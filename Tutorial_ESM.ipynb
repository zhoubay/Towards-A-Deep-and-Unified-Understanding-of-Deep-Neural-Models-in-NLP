{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the ESM model\n",
    "# model, alphabet = esm.pretrained.load_model_and_alphabet(\"/root/models/esm2_t6_8M_UR50D.pt\")\n",
    "# model, alphabet = esm.pretrained.load_model_and_alphabet(\"/root/models/esm2_t48_15B_UR50D.pt\")\n",
    "# model, alphabet = esm.pretrained.load_model_and_alphabet(\"/root/models/esm2_t12_35M_UR50D.pt\")\n",
    "model, alphabet = esm.pretrained.load_model_and_alphabet(\"/root/models/esm2_t30_150M_UR50D.pt\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = \"MRNPTLLQCFHWYYPEGGKLWPELAERADGFNDIGINMVWLPPAYKGASGGYSVGYDSYDLFDLGEFDQKGSIPTKYGDKAQLLAAIDALKRNDIAVLLDVVVNHKMGADEKEAIRVQRVNADDRTQIDEEIIECEGWTRYTFPARAGQYSQFIWDFKCFSGIDHIENPDEDGIFKIVNDYTGEGWNDQVDDELGNFDYLMGENIDFRNHAVTEEIKYWARWVMEQTQCDGFRLDAVKHIPAWFYKEWIEHVQEVAPKPLFIVAEYWSHEVDKLQTYIDQVEGKTMLFDAPLQMKFHEASRMGRDYDMTQIFTGTLVEADPFHAVTLVANHDTQPLQALEAPVEPWFKPLAYALILLRENGVPSVFYPDLYGAHYEDVGGDGQTYPIDMPIIEQLDELILARQRFAHGVQTLFFDHPNCIAFSRSGTDEFPGCVVVMSNGDDGEKTIHLGENYGNKTWRDFLGNRQERVVTDENGEATFFCNGGSVSVWVIEEVI\"\n",
    "sequence = \"FFSPSPARKRHAPSPEPAVQGTGVAGVPEESGDAAAIPAKKAPAGQEEPGTPPSSPLSAEQLDRIQRNKAAALLRLAARNVPVGFGESWKKHLSGEFGKPYFIKLMGFVAEERKHYTVYPPPHQVFTWTQMCDIKDVKVVILGQDPAHGPNQAHGLCFSVQRPVPPPPSLENIYKELSTDIEDFVHPGHGDLSGWAKQGVLLLNAVLTVRAHQANSHKERGWEQFTDAVVSWLNQNSNGLVFLLWGSYAQKKGSAIDRKKHHVLQTAHPSPLSVYRGFFGCRHFSKTNELLQKSGKKPIDWKEL\"\n",
    "_, _, tokens = converter([(\"amyA\", sequence)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.model.esm2 import ESM2\n",
    "\n",
    "class MyESM2(ESM2):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, x, repr_layers=[], need_head_weights=False, return_contacts=False, padding_mask=None, token_mask=None):\n",
    "        return self.forward_without_embedding(x, padding_mask, token_mask, repr_layers, need_head_weights, return_contacts)\n",
    "\n",
    "    def forward_without_embedding(self, x_raw, padding_mask, token_mask, repr_layers=[], need_head_weights=False, return_contacts=False):\n",
    "        if return_contacts:\n",
    "            need_head_weights = True\n",
    "\n",
    "        x = self.embed_scale * x_raw\n",
    "\n",
    "        if self.token_dropout:\n",
    "            x.masked_fill_(token_mask.unsqueeze(-1), 0.0)\n",
    "            # x: B x T x C\n",
    "            mask_ratio_train = 0.15 * 0.8\n",
    "            src_lengths = (~padding_mask).sum(-1)\n",
    "            mask_ratio_observed = token_mask.sum(-1).to(x.dtype) / src_lengths\n",
    "            x = x * (1 - mask_ratio_train) / (1 - mask_ratio_observed)[:, None, None]\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))\n",
    "\n",
    "        repr_layers = set(repr_layers)\n",
    "        hidden_representations = {}\n",
    "        if 0 in repr_layers:\n",
    "            hidden_representations[0] = x\n",
    "\n",
    "        if need_head_weights:\n",
    "            attn_weights = []\n",
    "\n",
    "        # (B, T, E) => (T, B, E)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        if not padding_mask.any():\n",
    "            padding_mask = None\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            # print(x)\n",
    "            x, attn = layer(\n",
    "                x,\n",
    "                self_attn_padding_mask=padding_mask,\n",
    "                need_head_weights=need_head_weights,\n",
    "            )\n",
    "            if (layer_idx + 1) in repr_layers:\n",
    "                hidden_representations[layer_idx + 1] = x.transpose(0, 1)\n",
    "            if need_head_weights:\n",
    "                # (H, B, T, T) => (B, H, T, T)\n",
    "                attn_weights.append(attn.transpose(1, 0))\n",
    "\n",
    "        x = self.emb_layer_norm_after(x)\n",
    "        x = x.transpose(0, 1)  # (T, B, E) => (B, T, E)\n",
    "\n",
    "        # last hidden representation should have layer norm applied\n",
    "        if (layer_idx + 1) in repr_layers:\n",
    "            hidden_representations[layer_idx + 1] = x\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        result = {\"logits\": x, \"representations\": hidden_representations}\n",
    "        if need_head_weights:\n",
    "            # attentions: B x L x H x T x T\n",
    "            attentions = torch.stack(attn_weights, 1)\n",
    "            if padding_mask is not None:\n",
    "                attention_mask = 1 - padding_mask.type_as(attentions)\n",
    "                attention_mask = attention_mask.unsqueeze(1) * attention_mask.unsqueeze(2)\n",
    "                attentions = attentions * attention_mask[:, None, None, :, :]\n",
    "            result[\"attentions\"] = attentions\n",
    "            # if return_contacts:\n",
    "            #     contacts = self.contact_head(tokens, attentions)\n",
    "            #     result[\"contacts\"] = contacts\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "mymodel = MyESM2(num_layers=model.num_layers, embed_dim=model.embed_dim, attention_heads=model.attention_heads, alphabet=model.alphabet, token_dropout=model.token_dropout)\n",
    "mymodel.load_state_dict(model.state_dict())\n",
    "mymodel = mymodel.to(device)\n",
    "for param in mymodel.parameters():\n",
    "    param.requires_grad = False\n",
    "mymodel.eval()\n",
    "\n",
    "words = [\"<bos>\"] + list(sequence) + [\"<eos>\"]\n",
    "x = mymodel.embed_tokens(tokens.to(device))\n",
    "padding_mask = tokens.eq(mymodel.padding_idx).to(device)  # B, T\n",
    "token_mask = (tokens == mymodel.mask_idx).to(device)\n",
    "dataset = [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phi(x):\n",
    "    global model, padding_mask, token_mask\n",
    "    # x = x.squeeze(0)\n",
    "    last_layer = len(mymodel.layers)\n",
    "    results = mymodel(x, padding_mask=padding_mask, token_mask=token_mask, repr_layers=[last_layer], return_contacts=False)\n",
    "    # print(results)\n",
    "    return results[\"representations\"][last_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Interpreter import Interpreter, calculate_regularization\n",
    "regularization = calculate_regularization(dataset, Phi, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = Interpreter(x=x, Phi=Phi, words=words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.optimize(iteration=5000, lr=0.01, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_ = interpreter.get_sigma()\n",
    "sigma_np = np.concatenate([sigma_, np.zeros((sigma_.shape[-1]//20 + 1)*20-sigma_.shape[-1])])\n",
    "words_new = words + [\"\"] * ((sigma_.shape[-1]//20 + 1)*20-sigma_.shape[-1])\n",
    "# print(sigma_np[:20])\n",
    "# print(len(words_new))\n",
    "sigma_np = sigma_np.reshape(20, -1)\n",
    "# print(sigma_np[0, :])\n",
    "# raise Exception\n",
    "# set figure size\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(sigma_np, cmap=\"GnBu_r\")\n",
    "# show color value\n",
    "for i in range(sigma_np.shape[0]): # 20\n",
    "    # print(i)\n",
    "    for j in range(sigma_np.shape[1]): # -1\n",
    "        plt.text(j, i, words_new[i*sigma_np.shape[1]+j]+str(i*sigma_np.shape[1]+j)+\"\\n\"+f\"{sigma_np[i][j]:.4f}\", ha=\"center\", va=\"center\", color=\"k\")\n",
    "# hide axis\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "# show color bar\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sigma_ = interpreter.get_sigma()\n",
    "sigma_ = sigma_.reshape(1, -1)\n",
    "plt.imshow([sigma_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordInformation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
